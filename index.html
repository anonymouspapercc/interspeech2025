<!DOCTYPE html>
<html lang="en-US">

<head>
    <title>Demo Page of REVC</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="style.css">
    <style>
                .method {
                        display: inline-block;
/*             width: 120px; /* Adjust the width as needed */ 
                        font-weight: bold;
                }

                .explanation {
                        display: inline-block;
/*             margin-left: 20px; /* Adjust the margin as needed */ 
                }
        </style>
</head>

<body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
    <section class="page-header">
    </section>
    <section class="main-content">
        <h1 id="">
            <center>REVC: Fast, Robust and Expressive Zero-Shot Voice Conversion with Diffusion Transformers</center>
        </h1>
        <h3 id="">
            <center>A work submitted to INTERSPEECH 2025</center>
        </h3>


        <br><br>
        <h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
        <p> In real-world voice conversion applications, environmental noise in source speech and user demands for expressive output pose critical challenges. Traditional ASR-based methods ensure noise robustness but suppress prosody, while SSL-based models improve expressiveness but suffer from timbre leakage and noise sensitivity. This paper proposes REVC, a noise-robust expressive voice conversion system. Key innovations include: (1) A random erasing strategy to mitigate the information redundancy inherent in SSL feature, enhancing noise robustness and expressiveness; (2) Implicit alignment inspired by E2TTS to suppress non-essential feature reconstruction; (3) Integration of Shortcut Models to accelerate flow matching inference, significantly reducing to 4 steps. Experimental results demonstrate that our model outperforms baselines such as Seed-VC in zero-shot scenarios on the noisy set, while also performing comparably to Seed-VC on the clean set. In addition, REVC can be compatible with singing voice conversion within one model. 
        <table frame=void rules=none>
            <tr>
                <center><img src='fig/interspeech2025-diffusionvc_overview.png' width="65%"></center>
                <center>Overview of REVC</center>
            </tr>
        </table>
        <br><br>


        <h2>2. Demos <a name="Comparison"></a></h2>
        <p>Compared Methods</p>
        <ul>
            <li><b>VITS-VC : a internal VITS-based VC system. </li> 
            <li><b><a href="https://github.com/Plachtaa/seed-vc">Seed-VC</a> : a diffusion transformer based VC system.</li>
        </ul>
        
        <h3>Clean Samples</h3>
        <table>
            <tbody id="tbody_clean">
            </tbody>
        </table>

        <h3>Noisy Samples</h3>
        <table style="margin-left: auto; margin-right: auto;">
            <tbody id="tbody_noisy">
            </tbody>
        </table>

        <h3>Singing Voice Conversion</h3>
        <table style="margin-left: auto; margin-right: auto;">
            <tbody id="tbody_svc">
            </tbody>
        </table>
        
    <h3>References:</h3>
    <div>
        <!-- <div><cite><a href="https://arxiv.org/pdf/2307.16430.pdf">
                [1] J. Kong, J. Park, B. Kim, J. Kim, D. Kong, and S. Kim, “VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design,” in Proc. INTERSPEECH 2023, 2023, pp. 4374–4378.</a></cite></div> -->
        <!-- <div><cite><a href="https://arxiv.org/pdf/2211.02336.pdf">
                [2] Detai Xin, Sharath Adavanne, Federico Ang, Ashish Kulkarni, Shinnosuke Takamichi, and Hiroshi Saruwatari,
                "Improving speech prosody of audiobook text-to-speech synthesis with acoustic and textual contexts," in
                Proc. ICASSP, 2023, pp. 1–5.</a></cite></div>
        <div><cite><a href="https://arxiv.org/pdf/2203.12201.pdf">
                [3] Shun Lei, Yixuan Zhou, Liyang Chen, Zhiyong Wu, Shiyin Kang, and Helen Meng, "Towards expressive speaking
                style modelling with hierarchical context information for Mandarin speech synthesis," in Proc. ICASSP
                2022, 2022, pp. 7922–7926.</a></cite></div> -->
    </div>


</html>

<script type="" text="javascript">
    function noisy() {
        let scenes = [
            [1],
            [2]
        ]
        let models = [["Prompt", "prompt"], ["Source", "source"], ["VITS-VC", "vitsvc"],  ["Seed-VC", "seedvc"], ["REVC (32NFE)", "revc32"], ["REVC (4NFE)", "revc4"]]
        let short_data = ""
                // `
                // <tr>
                //     <td style="text-align: center; " rowspan=1><strong>Text<strong></td>
                //     <td style="text-align: center; " rowspan=1><strong>English translation<strong></td>
                // `
        for (const id in models) {
            model = models[id][0]
            short_data += '<td style="text-align: center; width: 14%;" rowspan=1><strong>' + model  + '<strong></td>'
        }
        short_data += `</tr>`
        
        for (let x in scenes) {
            let scene = scenes[x]
            let file = scene[0]
            let scene_data = ""

            scene_data += '<tr>'
            // scene_data += '<td style="text-align: center; width: 10%;" rowspan=1>' + text + '</td>'
            // scene_data += '<td style="text-align: center; width: 10%;font-size:14px" rowspan=1>' + translation + '</td>'
            for (let z in models) {
                let model = models[z][1]
                scene_data += '<td style="text-align: center"><audio style="width: 100%;" controls="" src="' + './demos/' + model + '/' + file + '.wav' + '"></audio></td>'
                
            }
            scene_data += '</tr>'
            short_data += scene_data
        }
        return short_data
    }

    function clean() {
        let scenes = [
            [1],
            [2]
        ]
        let models = [["Prompt", "prompt"], ["Source", "source"], ["VITS-VC", "vitsvc"],  ["Seed-VC", "seedvc"], ["REVC (32NFE)", "revc32"], ["REVC (4NFE)", "revc4"]]
        let short_data = ""
                // `
                // <tr>
                //     <td style="text-align: center; " rowspan=1><strong>Text<strong></td>
                //     <td style="text-align: center; " rowspan=1><strong>English translation<strong></td>
                // `
        for (const id in models) {
            model = models[id][0]
            short_data += '<td style="text-align: center; width: 14%;" rowspan=1><strong>' + model  + '<strong></td>'
        }
        short_data += `</tr>`
        
        for (let x in scenes) {
            let scene = scenes[x]
            let file = scene[0]
            // let text = scene[1]
            // let traslation = scene[2]
            let scene_data = ""

            scene_data += '<tr>'
            // scene_data += '<td style="text-align: center; width: 10%;" rowspan=1>' + text + '</td>'
            // scene_data += '<td style="text-align: center; width: 10%;font-size:14px" rowspan=1>' + traslation + '</td>'
            for (let z in models) {
                let model = models[z][1]
                scene_data += '<td style="text-align: center"><audio style="width: 100%;" controls="" src="' + './demos/' + model + '/' + file + '.wav' + '"></audio></td>'
                
            }
            scene_data += '</tr>'
            short_data += scene_data
        }
        return short_data
    }

    function svc() {
        let scenes = [
            [1],
            [2]
        ]
        let models = [["Prompt", "prompt"], ["Source", "source"], ["REVC (32NFE)", "revc32"]]
        let short_data = ""
                // `
                // <tr>
                //     <td style="text-align: center; " rowspan=1><strong>Text<strong></td>
                //     <td style="text-align: center; " rowspan=1><strong>English translation<strong></td>
                // `
        for (const id in models) {
            model = models[id][0]
            short_data += '<td style="text-align: center; width: 14%;" rowspan=1><strong>' + model  + '<strong></td>'
        }
        short_data += `</tr>`
        
        for (let x in scenes) {
            let scene = scenes[x]
            let file = scene[0]
            // let text = scene[1]
            // let traslation = scene[2]
            let scene_data = ""

            scene_data += '<tr>'
            // scene_data += '<td style="text-align: center; width: 10%;" rowspan=1>' + text + '</td>'
            // scene_data += '<td style="text-align: center; width: 10%;font-size:14px" rowspan=1>' + traslation + '</td>'
            for (let z in models) {
                let model = models[z][1]
                scene_data += '<td style="text-align: center"><audio style="width: 100%;" controls="" src="' + './demos/' + model + '/' + file + '.wav' + '"></audio></td>'
                
            }
            scene_data += '</tr>'
            short_data += scene_data
        }
        return short_data
    }
    window.onload = function () {
        document.getElementById('tbody_clean').innerHTML = clean()
        document.getElementById('tbody_noisy').innerHTML = noisy()
        document.getElementById('tbody_svc').innerHTML = svc()
    }
</script> 
